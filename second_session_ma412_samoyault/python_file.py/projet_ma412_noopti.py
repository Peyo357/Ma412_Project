# -*- coding: utf-8 -*-
"""projet_Ma412_noopti.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_nTyZi0rjTMEyp2gWRxgsRcJNdh0ZGwP
"""

# First we import our libraries, we must use sklearn for our classification problem.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

#Load the two CSV as dataframe.
train_data = pd.read_csv('/content/drive/MyDrive/RATTRAPAGE_MA412/Ma412/Train_Test_data/train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/RATTRAPAGE_MA412/Ma412/Train_Test_data/test.csv')
train_head = train_data.head()
test_head = test_data.head()

test_head

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load your dataset into a pandas DataFrame
data = pd.read_csv("/content/drive/MyDrive/RATTRAPAGE_MA412/Ma412/Train_Test_data/train.csv")  # Replace with your data loading method

# Separate numerical and categorical columns
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = data.select_dtypes(include=['object']).columns

# Handle missing values for numerical columns
data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())


# Encode categorical variables
label_encoders = {}
for column in categorical_cols:
  le = LabelEncoder()
  data[column] = le.fit_transform(data[column])
  label_encoders[column] = le

# Calculate the correlation matrix
correlation_matrix = data.corr()

# **Solution 1: Using plt.subplots**
# Create a figure of desired size
fig, ax = plt.subplots(figsize=(30, 30))  # Adjust width and height

# Create the heatmap using Seaborn on the created axis
sns.heatmap(correlation_matrix, annot=True, ax=ax)

# **Solution 2: Using fig.set_size_inches (uncomment if preferred)**
# # Create the heatmap using Seaborn
# heatmap = sns.heatmap(correlation_matrix, annot=True)
#
# # Get the current figure
# fig = heatmap.get_figure()
#
# # Set figure size
# fig.set_size_inches(10, 10)  # Adjust width and height

# Customize the plot (optional)
plt.title("Correlation Heatmap")
plt.show()

#Short analysis of the content of the dataframes

#Train dataset

train_info = train_data.info()
train_nul= train_data.isnull().sum()
train_na= train_data.isna().sum()

train_na
#train_nul
#train_info
train_data.shape

# Test dataset

test_info = test_data.info()
test_head = test_data.head()
test_na = test_data.isna().sum()
test_nul= test_data.isnull().sum()

#test_info
test_na
#test_nul
test_data.shape

'''
With this basic exploration, we have now a better view of the problem. It's a binary classification
We must use the random Forest classifier.
For that we need to fill the N/A value, and encode values that have an object type.

Moreover, we have identify some features that are not relevant for our problem.

The unamed column is not usefull as it's the index of the dataframe, then the id column is useless, as it
just an identifaction value. Finnaly the last one is the Customer type, to fidelise a customer, it's necessary
to satisfy him and it's the goal of the problem. It's the consequence of the satisfaction score so
it must biased on the other features.
'''

#Drop irrelevant features
'''
train_data = train_data.drop(columns=['id'])
test_data = test_data.drop(columns=['id'])

train_data = train_data.drop(columns=['Unnamed: 0'])
test_data = test_data.drop(columns=['Unnamed: 0'])

train_data = train_data.drop(columns=['Customer Type'])
test_data = test_data.drop(columns=['Customer Type'])
'''

# Separate numerical and categorical columns
numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = train_data.select_dtypes(include=['object']).columns

# Handle missing values for numerical columns
train_data[numerical_cols] = train_data[numerical_cols].fillna(train_data[numerical_cols].mean())
test_data[numerical_cols] = test_data[numerical_cols].fillna(test_data[numerical_cols].mean())


# Encode categorical variables
label_encoders = {}
for column in categorical_cols:
    le = LabelEncoder()
    train_data[column] = le.fit_transform(train_data[column])
    test_data[column] = le.transform(test_data[column])
    label_encoders[column] = le



# Separate features and target

X_train = train_data.drop('satisfaction', axis=1)
y_train = train_data['satisfaction']
X_test = test_data.drop('satisfaction', axis=1)
y_test = test_data['satisfaction']
# Align test data with training data columns
X_test = X_test[X_train.columns]

# Create a Random Forest classifier
rfecv = RFECV(estimator=RandomForestClassifier(n_estimators=40, max_depth=6, random_state=42),step=2, cv=5, scoring='accuracy')

# Fit the RFECV model to the training data
rfecv.fit(X_train, y_train)

# Plot the number of features vs. cross-validation scores
print("Optimal number of caracteristiques: %d" % rfecv.n_features_)
print("Selected caracteristique: %s" % list(X_train.columns[rfecv.support_]))

# Train of the model

rf_model= RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)
print("Validation Accuracy with RFE:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['Neutral/Unsatisfied', 'Satisfied'], yticklabels=['Neutral/Unsatisfied', 'Satisfied'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()