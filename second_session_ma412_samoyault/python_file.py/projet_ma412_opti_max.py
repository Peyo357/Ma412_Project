# -*- coding: utf-8 -*-
"""projet_ma412_opti_max.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16uslr0JvSkaFCp7glfPgJ_v8E83QbVT5

#----------------------------------------------------
#            Projet de rattrapage Ma412
#----------------------------------------------------

#Samoyault Pierre SM1
"""

# First we import our libraries, we must use sklearn for our classification problem.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import RFECV
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

#Load the two CSV as dataframe.
train_data = pd.read_csv('/content/drive/MyDrive/RATTRAPAGE_MA412/Ma412/Train_Test_data/train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/RATTRAPAGE_MA412/Ma412/Train_Test_data/test.csv')
train_head = train_data.head()
test_head = test_data.head()

test_head

train_head = train_data.head()
train_head

#Short analysis of the content of the dataframes

#Train dataset

train_info = train_data.info()
train_nul= train_data.isnull().sum()
train_na= train_data.isna().sum()

train_na
#train_nul
#train_info
train_data.shape

# Test dataset

test_info = test_data.info()
test_head = test_data.head()
test_na = test_data.isna().sum()
test_nul= test_data.isnull().sum()

#test_info
test_na
#test_nul
test_data.shape

'''
With this basic exploration, we have now a better view of the problem. It's a binary classification
We must use the random Forest classifier.
For that we need to fill the N/A value, and encode values that have an object type.

Moreover, we have identify some features that are not relevant for our problem.

The unamed column is not usefull as it's the index of the dataframe, then the id column is useless, as it
just an identifaction value. Finnaly the last one is the Customer type, to fidelise a customer, it's necessary
to satisfy him and it's the goal of the problem. It's the consequence of the satisfaction score so
it must biased on the other features.
'''

#Drop irrelevant features

train_data = train_data.drop(columns=['id'])
test_data = test_data.drop(columns=['id'])

train_data = train_data.drop(columns=['Unnamed: 0'])
test_data = test_data.drop(columns=['Unnamed: 0'])

train_data = train_data.drop(columns=['Customer Type'])
test_data = test_data.drop(columns=['Customer Type'])

# Separate numerical and categorical columns
numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = train_data.select_dtypes(include=['object']).columns

# Handle missing values for numerical columns
train_data[numerical_cols] = train_data[numerical_cols].fillna(train_data[numerical_cols].mean())
test_data[numerical_cols] = test_data[numerical_cols].fillna(test_data[numerical_cols].mean())


# Encode categorical variables
label_encoders = {}
for column in categorical_cols:
    le = LabelEncoder()
    train_data[column] = le.fit_transform(train_data[column])
    test_data[column] = le.transform(test_data[column])
    label_encoders[column] = le

# Separate features and target

X_train = train_data.drop('satisfaction', axis=1)  # Replace 'target_column' with your actual target column name
y_train = train_data['satisfaction']  # Replace 'target_column' with your actual target column name
X_test = test_data.drop('satisfaction', axis=1)  # Ensure test data has the same structure, if applicable
y_test = test_data['satisfaction']  # Replace 'target_column' with your actual target column name

# Align test data with training data columns
X_test = X_test[X_train.columns]

# Use RFECV to select features with a simpler model

# Create a Random Forest classifier
rfecv = RFECV(estimator=RandomForestClassifier(n_estimators=40, max_depth=6, random_state=42),step=2, cv=5, scoring='accuracy')

# Fit the RFECV model to the training data
rfecv.fit(X_train, y_train)

# Plot the number of features vs. cross-validation scores
print("Optimal number of caracteristiques: %d" % rfecv.n_features_)
print("Selected caracteristique: %s" % list(X_train.columns[rfecv.support_]))

# Get the ranking of the features
feature_rankings = rfecv.ranking_
feature_names = X_train.columns

feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Ranking': feature_rankings
}).sort_values(by='Ranking', ascending=True)

print(feature_importance_df)
selected_features = feature_importance_df[feature_importance_df['Ranking'] == 1]

# Transform data according to selected characteristics
selected_features = X_train.columns[rfecv.support_]
X_train_rfe = X_train[selected_features]
X_test_rfe = X_test[selected_features]

# Define the parameter grid to be tested
param_grid = {
    'estimator__n_estimators': [50],
    'estimator__max_depth': [None,2,5],
    'estimator__min_samples_split': [2, 5],
    'estimator__min_samples_leaf': [1, 2],

}

# Configure grid search with a simple split test train
grid_search = GridSearchCV(estimator=rfecv, param_grid=param_grid, cv=2, n_jobs=-1, verbose=2)

# Run a grid search on all the data
grid_search.fit(X_train_rfe, y_train)
best_params = grid_search.best_params_

# Best parameters found by grid search
print("Best parameters founds: ", best_params)

# Prédire et évaluer avec le meilleur modèle trouvé
best_rf = grid_search.best_estimator_
y_test_pred_rfe = best_rf.predict(X_test_rfe)

# Calculer les métriques de performance
print("Accuracy: ", accuracy_score(y_test, y_test_pred_rfe))
print(classification_report(y_test, y_test_pred_rfe))

#Calculate the confusion matrix

target_encoder = LabelEncoder()
train_data['satisfaction'] = target_encoder.fit_transform(train_data['satisfaction'])
test_data['satisfaction'] = target_encoder.transform(test_data['satisfaction'])
conf_matrix = confusion_matrix(y_test, y_test_pred_rfe)
print("Matrice de confusion:\n", conf_matrix)

# Displey the matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=target_encoder.classes_, yticklabels=target_encoder.classes_)
plt.xlabel('Pred')
plt.ylabel('Real')
plt.title('Confusion matrix')
plt.show()